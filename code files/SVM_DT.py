# -*- coding: utf-8 -*-
"""FinalCode_IR_Project_Murali.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ITMrxN1PMmMa72H7hRSTGZUujafWVJ_p
"""

import pandas as pd
from nltk.stem import PorterStemmer 
ps = PorterStemmer()
from tqdm import tqdm

import matplotlib.pyplot as plt
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier

from google.colab import drive
drive.mount('/content/drive')

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

data_path = '/content/drive/My Drive/IR Project/facts_opinions.csv'

data = pd.read_csv(data_path)
data.head()

total_data = pd.concat([opinions, facts], axis = 0).reset_index(drop=True)

label ={0 : 'Opinion', 1 : 'Facts'}
label

def preprocess(text):

    symbols = list(',.!@#$%^&*()')
    mod_words = []
    words = text.split()
    for word in words :
        word = word.lower()
        for sym in symbols:
            if sym in word:
                word = word.replace(sym, '')
        if word not in stop_words and word.isalpha() :
            word = ps.stem(word)
            if len(word) > 2:
                mod_words.append(word)

    text = ' '.join(mod_words)
    return text

total_data['Text'] = total_data['Text'].apply(preprocess) # pre processing of data
total_data = total_data.sample(frac=1).reset_index(drop=True) #shuffling the data

"""## Decision Tree

Splitting the Data
"""

Xtrain, Xtest, ytrain, ytest = train_test_split(total_data['Text'], total_data['Label'], test_size=0.3, random_state=42)

Xtrain, Xval, ytrain, yval = train_test_split(Xtrain, ytrain, test_size = 0.2, random_state = 40)

"""BOW Embedding"""

from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer(analyzer='word',max_features = 50,stop_words = 'english')
Xtrain_bow= vec.fit_transform(Xtrain)
Xtrain_bow = Xtrain_bow.toarray()

Xval_bow = vec.transform(Xval).toarray()

Xtest_bow = vec.transform(Xtest)
Xtest_bow = Xtest_bow.toarray()

"""### Decision Tree Implementation"""

max_depth = [1,2,3,4,5,6,7,8]
accuracy = []
f_scores = []
precisions = []
recall = []
for k in max_depth:

    classifier = DecisionTreeClassifier(max_depth=k)
    classifier.fit(Xtrain_bow, ytrain)
    ypred = classifier.predict(Xval_bow) # validation data
    acc = accuracy_score(yval, ypred)
    f_score = f1_score(yval, ypred, average='macro')
    prec = precision_score(yval, ypred, average='macro')
    rec = recall_score(yval, ypred, average='macro')
    accuracy.append(acc)
    f_scores.append(f_score)
    precisions.append(prec)
    recall.append(rec)

    print(f'Max Depth : {k} Accuracy : {acc} Precision : {prec} Recall : {rec} F score : {f_score}')

plt.plot(accuracy,label = 'Accuracy')
plt.plot(f_scores,label = 'F-score')
plt.plot(precisions,label = 'Precision')
plt.plot(recall,label = 'Recall')
y_pos = np.arange(len(max_depth))
plt.xticks(y_pos,max_depth)
plt.xlabel('Max Depth Value')
plt.legend()
plt.title('Decision Tree on Validation Data (BOW Approach)')
plt.show()

"""Best Decision Tree for BOW : Max_depth = 7"""

#Best Max_Depth is 7

best_dt_clf = DecisionTreeClassifier(max_depth=7)
best_dt_clf.fit(Xtrain_bow, ytrain)

ypred = best_dt_clf.predict(Xtest_bow) # Predicting Test data

acc = accuracy_score(ytest, ypred)
f_score = f1_score(ytest, ypred, average='macro')
prec = precision_score(ytest, ypred, average='macro')
rec = recall_score(ytest, ypred, average='macro')


print(f'Max Depth : {6} Accuracy : {acc} Precision : {prec} Recall : {rec} F score : {f_score}')

"""TFIDF Word Embedding"""

vectorizer = TfidfVectorizer(min_df = 0.01,max_features = 50)

Xtrain_tfidf = vectorizer.fit_transform(Xtrain).todense()
Xval_tfidf = vectorizer.transform(Xval).todense()
Xtest_tfidf = vectorizer.transform(Xtest).todense()

max_depth = [1,2,3,4,5,6,7,8,9]
accuracy_tfidf = []
f_scores_tfidf = []
precisions_tfidf = []
recall_tfidf = []
for k in max_depth:

    classifier = DecisionTreeClassifier(max_depth=k)
    classifier.fit(Xtrain_tfidf, ytrain)
    ypred = classifier.predict(Xval_tfidf) # validation data
    acc = accuracy_score(yval, ypred)
    f_score = f1_score(yval, ypred, average='macro')
    prec = precision_score(yval, ypred, average='macro')
    rec = recall_score(yval, ypred, average='macro')
    
    accuracy_tfidf.append(acc)
    f_scores_tfidf.append(f_score)
    precisions_tfidf.append(prec)
    recall_tfidf.append(rec)

    print(f'K : {k} Accuracy : {acc} Precision : {prec} Recall : {rec} F score : {f_score}')

plt.plot(accuracy_tfidf,label = 'Accuracy')
plt.plot(f_scores_tfidf,label = 'F-scores')
plt.plot(precisions_tfidf,label = 'Precisions')
plt.plot(recall_tfidf,label = 'Recall')
y_pos = np.arange(len(max_depth))
plt.xticks(y_pos,max_depth)
plt.xlabel('Max Depth Value')
plt.legend()
plt.title('Decision Tree on Validation Data (TFIDF Approach)')
# plt.savefig('bow_Accuracy.png')
plt.show()

"""Best Decision Tree for TFIDF : Max_depth = 8"""

best_knn_tfidf = DecisionTreeClassifier(max_depth=8)
best_knn_tfidf.fit(Xtrain_tfidf, ytrain)

ypred = best_knn_tfidf.predict(Xtest_tfidf) # Predicting Test data

acc = accuracy_score(ytest, ypred)
f_score = f1_score(ytest, ypred, average='macro')
prec = precision_score(ytest, ypred, average='macro')
rec = recall_score(ytest, ypred, average='macro')


print(f'K : {8} Accuracy : {acc} Precision : {prec} Recall : {rec} F score : {f_score}')

"""## SVM"""

# Xtrain, Xtest, ytrain, ytest

from sklearn import svm
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

"""Train Test Split"""

Xtrain_svm, Xtest_svm, ytrain_svm, ytest_svm = train_test_split(total_data['Text'], total_data['Label'], test_size=0.3, random_state=42)



"""BOW Embedding"""

vec = CountVectorizer(analyzer='word',max_features = 50,stop_words = 'english')

Xtrain_svm_bow= vec.fit_transform(Xtrain_svm)
Xtrain_svm_bow = Xtrain_svm_bow.toarray()

Xtest_svm_bow = vec.transform(Xtest_svm)
Xtest_svm_bow = Xtest_svm_bow.toarray()

parameters = {'kernel':('linear', 'rbf'), 'C':[0.0001,0.001,0.01,1,10]}
model = svm.SVC()
clf_bow = GridSearchCV(model, parameters, scoring='accuracy',cv=10)
clf_bow.fit(Xtrain_svm_bow, ytrain_svm)

## Graph Plotting

list_score = list(clf_bow.cv_results_['mean_test_score'])
list_kernel = list(np.array(clf_bow.cv_results_['param_kernel']))
list_C = list(clf_bow.cv_results_['param_C'])

list_c = list(set(list_C))
list_c.sort()

list_kernel_linear = []
list_kernel_rbf = []

index = 0
for item in list_kernel:
  if item == 'linear':
    score = list_score[index]
    list_kernel_linear.append(score)
  else:
    score = list_score[index]
    list_kernel_rbf.append(score)
  index += 1

plt.plot(list_kernel_linear,label = 'Linear Kernel')
plt.plot(list_kernel_rbf,label = 'RBF Kernel')

y_pos = np.arange(len(list_c))
cval = ["0.0001","0.001","0.01","1","10"]
plt.xticks(y_pos,cval)

plt.xlabel('Value of C')
plt.legend()
plt.title('Score VS Parameter C (BOW Approach)')
plt.show()

print(" The Accuracy of the best model is : ",clf_bow.best_score_)
print(" Best Parameters : ",clf_bow.best_params_)

classifier_best = clf_bow.best_estimator_
classifier_best.fit(Xtrain_svm_bow, ytrain_svm)
Y_pred = classifier_best.predict(Xtest_svm_bow)

acc = accuracy_score(ytest_svm, Y_pred)
f_score = f1_score(ytest_svm, Y_pred, average='macro')
prec = precision_score(ytest_svm, Y_pred, average='macro')
rec = recall_score(ytest_svm, Y_pred, average='macro')

print(f' Accuracy : {acc} Precision : {prec} Recall : {rec} F score : {f_score}')





"""### TFIDF Embedding"""



vectorizer = TfidfVectorizer(min_df = 0.01,max_features = 50)

Xtrain_svm_tfidf = vectorizer.fit_transform(Xtrain_svm).todense()
Xtest_svm_tfidf = vectorizer.transform(Xtest_svm).todense()

parameters = {'kernel':('linear', 'rbf'), 'C':[0.0001,0.001,0.01,1,10]}
model = svm.SVC()
clf = GridSearchCV(model, parameters, scoring='accuracy',cv=10)
clf.fit(Xtrain_svm_tfidf, ytrain_svm)

## Graph Plotting

list_score_tfidf = list(clf.cv_results_['mean_test_score'])
list_kernel_tfidf = list(np.array(clf.cv_results_['param_kernel']))
list_C_tfidf = list(clf.cv_results_['param_C'])

list_c_tfidf = list(set(list_C_tfidf))
list_c_tfidf.sort()

list_kernel_linear_tfidf = []
list_kernel_rbf_tfidf = []

index = 0
for item in list_kernel_tfidf:
  if item == 'linear':
    score = list_score_tfidf[index]
    list_kernel_linear_tfidf.append(score)
  else:
    score = list_score_tfidf[index]
    list_kernel_rbf_tfidf.append(score)
  index += 1

plt.plot(list_kernel_linear_tfidf,label = 'Linear Kernel')
plt.plot(list_kernel_rbf_tfidf,label = 'RBF Kernel')

y_pos = np.arange(len(list_c))
cval = ["0.0001","0.001","0.01","1","10"]
plt.xticks(y_pos,cval)

plt.xlabel('Value of C')
plt.legend()
plt.title('Score VS Parameter C (TFIDF Approach)')
plt.show()

print(" The Accuracy of the best model is : ",clf.best_score_)
print(" Best Parameters : ",clf.best_params_)

classifier_best = clf.best_estimator_
classifier_best.fit(Xtrain_svm_tfidf, ytrain_svm)
Y_pred = classifier_best.predict(Xtest_svm_tfidf)

acc = accuracy_score(ytest_svm, Y_pred)
f_score = f1_score(ytest_svm, Y_pred, average='macro')
prec = precision_score(ytest_svm, Y_pred, average='macro')
rec = recall_score(ytest_svm, Y_pred, average='macro')

print(f' Accuracy : {acc} Precision : {prec} Recall : {rec} F score : {f_score}')









